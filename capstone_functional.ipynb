{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61741bba-2133-42e0-8063-cf04ba119f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries, packages, modules\n",
    "\n",
    "import string\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import nltk\n",
    "import syllapy as syl\n",
    "from nltk.tokenize import word_tokenize\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58df5f43-e7f2-4633-a611-1aaffb4fef5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for file handling\n",
    "\n",
    "def process_file(file):\n",
    "    \"\"\" Takes a .txt file as an argument and returns a string object.\"\"\"\n",
    "    txt_test = \"\"\n",
    "    input_fd = open(file, \"r\", errors='ignore')\n",
    "    for line_str in input_fd:\n",
    "#         line_str = line_str.decode('utf-8',errors='ignore')\n",
    "        line_str = line_str.strip() # remove the carriage return\n",
    "        txt_test += \" \"\n",
    "        txt_test += line_str\n",
    "\n",
    "    input_fd.close()\n",
    "    return txt_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53d07121-9760-4a24-b035-16704ba608cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining functions to get features from the string object\n",
    "# Note: nltk tokenizer counts sentence-boundaries as tokens. The following functions do not.\n",
    "\n",
    "def get_tokens(text):\n",
    "    \"\"\"Takes a string as an argument, returns a list of lower-case words.\n",
    "    Contractions are included as words.\"\"\"\n",
    "    translator = str.maketrans('', '', \".!?,;:\")\n",
    "    stripped = text.translate(translator) # removes punctuation\n",
    "    lower_words = stripped.lower() # converts all alpha characters to lowercase\n",
    "    tokens = lower_words.split() # splits on whitespace\n",
    "    return tokens\n",
    "\n",
    "def get_types(tokens):\n",
    "    types = set(tokens) # creates a set of unique words\n",
    "    return types\n",
    "\n",
    "def count_tokens(text):\n",
    "    \"\"\"Takes a string as an argument, returns the number of total words.\"\"\"\n",
    "    tokens = get_tokens(text)\n",
    "    token_ct = len(tokens) # counts total words\n",
    "    return token_ct\n",
    "\n",
    "# functions for counting characters and syllables\n",
    "\n",
    "def count_char(tokens):\n",
    "    \"\"\"Takes a list of words as an argument. Returns the total number of characters in the words.\"\"\"\n",
    "    char_ct = 0\n",
    "    for token in tokens:\n",
    "        for char in token:\n",
    "            char_ct += 1\n",
    "    return char_ct\n",
    "\n",
    "def avg_token_len(text):\n",
    "    \"\"\"Take a string as an argument. Return the average length of a token in characters.\"\"\"\n",
    "    tokens = get_tokens(text)\n",
    "    char_ct = count_char(tokens)\n",
    "    avg_token_len = char_ct / len(tokens)\n",
    "    return avg_token_len\n",
    "\n",
    "def get_syllables(text):\n",
    "    \"\"\"Take a string as an argument. Return the total syllable count of the text.\"\"\"\n",
    "    tokens = get_tokens(text)\n",
    "    syllable_ct = 0\n",
    "    for token in tokens:\n",
    "        syllable_ct += syl.count(token)\n",
    "    return syllable_ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2367ab40-fd08-429d-b723-aaa90db41708",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def break_sentences(text):\n",
    "    \"\"\"Takes a string as an argument, returns a list of sentences.\"\"\"\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    return sentences\n",
    "\n",
    "def sentence_count(text):\n",
    "    \"\"\"Takes a string as an argument, returns the number of sentences in the string.\"\"\"\n",
    "    sentences = break_sentences(text)\n",
    "    return len(sentences)\n",
    "\n",
    "def avg_sent_length(text):\n",
    "    \"\"\"Takes a string as an argument, returns the average length of the sentences in the string.\"\"\"\n",
    "    sentence_ct = sentence_count(text)\n",
    "    word_ct = count_tokens(text)\n",
    "    asl = word_ct / sentence_ct\n",
    "    return asl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "874c8a64-3fb3-44f0-bb3d-ed560efac791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions to tag POS with nltk\n",
    "# and to get features related to POS categories\n",
    "\n",
    "def get_pos_dict(text):\n",
    "    \"\"\"A function to return a dictionary containing the number of instances of each pos category in a text.\"\"\"\n",
    "    tag_ct_dict = {}\n",
    "    sents = break_sentences(text) # get sentences\n",
    "    for sent in sents:\n",
    "        tokens = get_tokens(sent)\n",
    "        tagged_tokens = nltk.pos_tag(tokens)\n",
    "        for token, tag in tagged_tokens:\n",
    "            if tag in tag_ct_dict:\n",
    "              tag_ct_dict[tag] +=1\n",
    "            else:\n",
    "              tag_ct_dict[tag] = 1\n",
    "    return tag_ct_dict\n",
    "\n",
    "# this function needs to be re-written to take a dictionary as argument\n",
    "def pos_total_cats(my_dict):\n",
    "    \"\"\"Takes a dictionary as an argument. Returns the length of the dictionary.\"\"\"\n",
    "    pos_cat_count = len(my_dict) # simply counts how many different POS categories are present in the text\n",
    "    return pos_cat_count\n",
    "\n",
    "def tag_in_text(tag, my_dict):\n",
    "    \"\"\"This function takes two arguments - a particular tag, and the extracted POS dictionary of the text.\n",
    "    Returns the count of the tag\"\"\"\n",
    "    if tag in my_dict.keys():\n",
    "        ct = my_dict.get(tag)\n",
    "        return ct\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_full_dict(my_dict):\n",
    "    \"\"\"Takes the dictionary directly extracted by the POS tagger and returns a 'full' dictionary.\n",
    "    * Important! *\n",
    "    This function relies on the tag_in_text function.\"\"\"\n",
    "    penn_tags_list = ['CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', \\\n",
    "                      'LS', 'MD', 'NN', 'NNS', 'NNP', 'NNPS', 'PDT', 'POS', 'PRP', \\\n",
    "                      'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', \\\n",
    "                      'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB']\n",
    "    full_dict = {}\n",
    "    for tag in penn_tags_list:\n",
    "        ct = tag_in_text(tag, my_dict)\n",
    "        full_dict.update({tag : ct})\n",
    "    return full_dict\n",
    "\n",
    "def get_cat_dicts(full_dict):\n",
    "    \"\"\"This function returns the broad category counts.\n",
    "    Takes the 'full' dictionary as an argument and returns summed tallies of general pos categories.\"\"\"\n",
    "    jj_dict = dict(filter(lambda item: \"JJ\" in item[0], full_dict.items()))\n",
    "    nn_dict = dict(filter(lambda item: \"NN\" in item[0], full_dict.items()))\n",
    "    prp_dict = dict(filter(lambda item: \"PRP\" in item[0], full_dict.items()))\n",
    "    rb_dict = dict(filter(lambda item: \"RB\" in item[0], full_dict.items()))\n",
    "    vb_dict = dict(filter(lambda item: \"VB\" in item[0], full_dict.items()))\n",
    "    cpx_list = ['CC', 'IN', 'MD', 'TO', 'WDT', 'WP', 'WP$', 'WRB']\n",
    "    cpx_dict = {}\n",
    "    for tag in cpx_list:\n",
    "        ct = tag_in_text(tag, full_dict)\n",
    "        cpx_dict.update({tag : ct})\n",
    "    return jj_dict, nn_dict, prp_dict, rb_dict, vb_dict, cpx_dict\n",
    "\n",
    "def get_dict_value_ct(my_dict):\n",
    "    ct = 0\n",
    "    for key in my_dict.keys():\n",
    "        if my_dict.get(key) != 0:\n",
    "            ct += 1\n",
    "    return ct\n",
    "    \n",
    "def get_dict_value_sum(my_dict):\n",
    "    sum = 0\n",
    "    for key in my_dict.keys():\n",
    "        sum += my_dict.get(key)\n",
    "    return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "162ab90d-8bb6-4733-9f41-5e6b3d9bb8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This cell reads three csv files.\n",
    "They contain the academic word list and general service list.\n",
    "We load those into two dictionaries for feature extraction. \"\"\"\n",
    "\n",
    "import csv\n",
    "\n",
    "my_acad_dict = {}\n",
    "with open('eap_asl.csv', newline='') as csvfile:\n",
    "    csvreader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in csvreader:\n",
    "        my_acad_dict.update({row[0]:row[1]})\n",
    "csvfile.close()\n",
    "\n",
    "my_gsl_dict = {}\n",
    "\"\"\"This block of code imports the general service list.\n",
    "Each word has an integer frequency value. Higher values indicate\n",
    "more frequent (common) words. \"\"\"\n",
    "with open('gsl_final.csv', newline='') as csvfile:\n",
    "    csvreader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in csvreader:\n",
    "        my_gsl_dict.update({row[0]:row[1]})\n",
    "csvfile.close()\n",
    "\n",
    "longman_dict = {}\n",
    "\"\"\"This block of code imports the Longman 3000.\n",
    "Each word has an integer frequency value. Higher values indicate\n",
    "more frequent (common) words. \"\"\"\n",
    "with open('longman_final.csv', newline='') as csvfile:\n",
    "    csvreader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in csvreader:\n",
    "        longman_dict.update({row[0]:row[1]})\n",
    "csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41aa69fa-60c8-4f8a-b3c2-7aedcc06aa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell defines five functions to handle academic and general service vocabulary in the text.\n",
    "\n",
    "def acad_ct(my_list, acad_dict):\n",
    "    \"\"\"This function counts how many academic words are present in a list.\"\"\"\n",
    "    acad_ct = 0\n",
    "    for item in my_list:\n",
    "        if item in acad_dict.keys():\n",
    "            acad_ct += 1\n",
    "    return acad_ct\n",
    "\n",
    "def acad_in_list(my_list, acad_dict):\n",
    "    \"\"\"This function returns a list of academic words which apepar in the list.\n",
    "    Will work best when the list argument is the types of a given text.\n",
    "    The academic word list has both head-words and inflected forms,\n",
    "    This version of the function returns only the head-word.\"\"\"\n",
    "    output_list = []\n",
    "    for item in my_list:\n",
    "        if item in acad_dict.keys():\n",
    "            output_list.append(acad_dict.get(item))\n",
    "    output_set = set(output_list)\n",
    "    return sorted(output_set)\n",
    "\n",
    "def gsl_simple_ct(my_list, gsl_dict):\n",
    "    \"\"\"Return a simple count of how many list items are in the gsl.\"\"\"\n",
    "    gsl_ct = 0\n",
    "    for item in my_list:\n",
    "        if item in gsl_dict.keys():\n",
    "            gsl_ct += 1\n",
    "    return gsl_ct\n",
    "\n",
    "def get_gsl_freq(my_list, gsl_dict):\n",
    "    \"\"\"This function gets the relative frequency of the items in the list.\n",
    "    It takes two arguments: a list and a dictionary.\n",
    "    Returns an integer value, the sum of the frequency values for each item.\n",
    "   \"\"\"\n",
    "    gsl_freq_sum = 0\n",
    "    for item in my_list:\n",
    "        if item in gsl_dict.keys():\n",
    "            gsl_freq_sum += (int(gsl_dict.get(item)))\n",
    "    return gsl_freq_sum\n",
    "\n",
    "def get_longman_freq(my_list, longman_dict):\n",
    "    longman_freq_sum = 0\n",
    "    for item in my_list:\n",
    "        if item in longman_dict.keys():\n",
    "            longman_freq_sum += (int(longman_dict.get(item)))\n",
    "    return longman_freq_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "357d10ef-4e64-4884-a24a-e7046ab3b2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This cell reads the csv file with the cognates and loads them into four dictionaries. \"\"\"\n",
    "\n",
    "en_es_dict = {}\n",
    "en_fr_dict = {}\n",
    "en_ht_dict = {}\n",
    "en_por_dict = {}\n",
    "\n",
    "with open('cognates_final.csv', newline='', encoding='utf-8') as csvfile:\n",
    "    reader = csv.DictReader(csvfile, restval='xxx')\n",
    "    for row in reader:\n",
    "        if row['Spanish'] != 'xxx':\n",
    "            es_cognate = row['Spanish']\n",
    "            if len(es_cognate) > 2:\n",
    "                en_es_dict.update({row['English'] : es_cognate})\n",
    "        if row['French'] != 'xxx':\n",
    "            fr_cognate = row['French']\n",
    "            if len(fr_cognate) > 2:\n",
    "                en_fr_dict.update({row['English'] : fr_cognate})\n",
    "        if row['Haitian Creole'] != 'xxx':\n",
    "            ht_cognate = row['Haitian Creole']\n",
    "            if len(ht_cognate) > 2:\n",
    "                en_ht_dict.update({row['English'] : ht_cognate})\n",
    "        if row['Portuguese'] != 'xxx':\n",
    "            por_cognate = row['Portuguese']\n",
    "            if len(por_cognate) > 2:\n",
    "                en_por_dict.update({row['English'] : por_cognate})\n",
    "csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a17bdfb2-ca2a-47dc-bf31-1d829b82bba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cognate_token_pct(text, cognate_dict):\n",
    "    \"\"\"Given a text and cognate dictionary as arguments, this function returns the percentage of TOKENS that are cognates\n",
    "    with that L1.\"\"\"\n",
    "    tokens = get_tokens(text)\n",
    "    token_ct = 0\n",
    "    cognate_ct = 0\n",
    "    for token in tokens:\n",
    "        token_ct += 1\n",
    "        if token in cognate_dict.keys():\n",
    "            cognate_ct += 1\n",
    "    return cognate_ct / token_ct\n",
    "\n",
    "def get_cognate_type_pct(text, cognate_dict):\n",
    "    \"\"\"Given a text and cognate dictionary as arguments, this function returns the percentage of TYPES that are cognates\n",
    "    with that L1.\"\"\"\n",
    "    tokens = get_tokens(text)\n",
    "    types = get_types(tokens)\n",
    "    type_ct = 0\n",
    "    cognate_ct = 0\n",
    "    for my_type in types:\n",
    "        type_ct += 1\n",
    "        if my_type in cognate_dict.keys():\n",
    "            cognate_ct += 1\n",
    "    return cognate_ct / type_ct\n",
    "\n",
    "def get_cognate_pct(my_list, cognate_dict):\n",
    "    \"\"\"Here the idea is to use a more versatile function that will work on either the tokens or the types.\"\"\"\n",
    "    item_ct = 0\n",
    "    cognate_ct = 0\n",
    "    for item in my_list:\n",
    "        item_ct += 1\n",
    "        if item in cognate_dict.keys():\n",
    "            cognate_ct += 1\n",
    "    return cognate_ct / item_ct\n",
    "\n",
    "def get_cognate_list(text, cognate_dict):\n",
    "    \"\"\"This function takes a text and cognate dictionary as arguments, returns a list of the cognates in the text.\"\"\"\n",
    "    tokens = get_tokens(text)\n",
    "    types = get_types(tokens)\n",
    "    my_cognate_list = []\n",
    "    for my_type in types:\n",
    "        if my_type in cognate_dict.keys():\n",
    "            baby_list = [my_type, cognate_dict.get(my_type)]\n",
    "            my_cognate_list.append(baby_list)\n",
    "    return my_cognate_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5729090e-f56c-42b1-9c61-f8b633781927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# legacy readability indices\n",
    "# these two use sentences, word count, and syllables\n",
    "\n",
    "def flesch_kincaid(text):\n",
    "    sent_ct = sentence_count(text)\n",
    "    tokens = get_tokens(text)\n",
    "    word_ct = len(tokens)\n",
    "    syllable_ct = get_syllables(text)\n",
    "    fkre = 206.835 - 1.015*(word_ct / sent_ct) - 84.6*(syllable_ct / word_ct)\n",
    "    if fkre > 0:\n",
    "        return fkre\n",
    "    else:\n",
    "        return 0\n",
    " \n",
    "def gunning_fog(text):\n",
    "    sent_ct = sentence_count(text)\n",
    "    tokens = get_tokens(text)\n",
    "    word_ct = len(tokens)\n",
    "    complex_ct = 0\n",
    "    for token in tokens:\n",
    "        syllable = syl.count(token)\n",
    "        if syllable > 2:\n",
    "            complex_ct +=1\n",
    "    gf_index = 0.4*((word_ct / sent_ct) + 100*(complex_ct / word_ct))\n",
    "    if gf_index > 0:\n",
    "        return gf_index\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# these two use sentences, word count, and characters\n",
    "\n",
    "def get_ari(text):\n",
    "    \"\"\"This one is called the automated readability index. Military, 1960s.\"\"\"\n",
    "    sent_ct = sentence_count(text)\n",
    "    tokens = get_tokens(text)\n",
    "    word_ct = len(tokens)\n",
    "    char_ct = count_char(tokens)\n",
    "    ari = 4.71*(char_ct / word_ct) + 0.5*(word_ct / sent_ct) - 21.43\n",
    "    if ari > 0:\n",
    "        return ari\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def coleman_liau(text):\n",
    "    \"\"\"The Coleman-Liau Formula is based on total characters per 100 words, sentences per 100 words, and a constant.\"\"\"\n",
    "    sent_ct = sentence_count(text)\n",
    "    tokens = get_tokens(text)\n",
    "    word_ct = len(tokens)\n",
    "    char_ct = count_char(tokens)\n",
    "    coleman_liau = 5.88*(char_ct / word_ct) - 29.6*(sent_ct / word_ct) - 15.8\n",
    "    if coleman_liau > 0:\n",
    "        return coleman_liau\n",
    "    else:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6a18a98-3d5f-4d71-bb34-6078b5dd0761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the functions above, create a 'list of lists' object containing features for each document.\n",
    "\n",
    "def get_features(text, L1_val=\"es\"):\n",
    "    \"\"\"Takes a long string as an argument, and returns a list of x features:\n",
    "       \"\"\"\n",
    "    feature_list = [] # initialize list object to receive features\n",
    "    # text = process_file(doc) # get the txt file as a string\n",
    "    \n",
    "    tokens = get_tokens(text)\n",
    "    types = get_types(tokens)\n",
    "    sentences = break_sentences(text)\n",
    "    \n",
    "    # get 6 basic features\n",
    "\n",
    "    token_ct = len(tokens)\n",
    "    type_ct = len(types)\n",
    "    # token_ct, type_ct = tokens_and_types(text) # get token and type counts\n",
    "    ttr = (type_ct / token_ct) # get type-to-token ratio\n",
    "    sent_ct = sentence_count(text) # get sentence count\n",
    "    asl = avg_sent_length(text) # get average sentence length\n",
    "    # max_sl = max_sent_length(text) # get max sentence length\n",
    "\n",
    "    # add 6 basic features\n",
    "    \n",
    "    # add feature token count\n",
    "    feature_list.append(token_ct)\n",
    "    # add feature type count\n",
    "    feature_list.append(type_ct)\n",
    "    # add feature ttr\n",
    "    feature_list.append(ttr)\n",
    "    # add feature sentence count\n",
    "    feature_list.append(sent_ct)\n",
    "    # add feature average sentence length\n",
    "    feature_list.append(asl)\n",
    "    # add feature max sentence length\n",
    "    # feature_list.append(max_sl)    \n",
    "    \n",
    "    # get pos dictionaries\n",
    "\n",
    "    my_dict = get_pos_dict(text)\n",
    "    full_dict = get_full_dict(my_dict)\n",
    "    jj_dict, nn_dict, prp_dict, rb_dict, vb_dict, cpx_dict = get_cat_dicts(full_dict)\n",
    "\n",
    "    # Use this dict to select features from full_dict of pos features\n",
    "    pos_light_switch = {'CC': 1, 'CD': 1, 'DT': 1, 'EX': 1, 'FW': 0, 'IN': 1, 'JJ': 1, 'JJR': 1, 'JJS': 1, \\\n",
    "                      'LS': 0, 'MD': 1, 'NN': 1, 'NNS': 1, 'NNP': 0, 'NNPS': 0, 'PDT': 0, 'POS': 0, 'PRP': 1, \\\n",
    "                      'PRP$': 0, 'RB': 1, 'RBR': 0, 'RBS': 0, 'RP': 1, 'SYM': 0, 'TO': 1, 'UH': 0, \\\n",
    "                      'VB': 1, 'VBD': 1, 'VBG': 1, 'VBN': 1, 'VBP': 1, 'VBZ': 0, 'WDT': 0, 'WP': 0, 'WP$': 0, 'WRB': 0}\n",
    "    \n",
    "    # get and append up to 36 pos counts as features\n",
    "    for key in full_dict.keys():\n",
    "        my_int = full_dict.get(key)\n",
    "        if pos_light_switch.get(key) == 1: # this line allows some but not all POS cats to be used as features, see above\n",
    "            feature_list.append(my_int)\n",
    "\n",
    "   \n",
    "    # get and append up to 20 aggregate pos features\n",
    "    pos_total_ct = get_dict_value_ct(full_dict) # number of syntactic categories present in the text\n",
    "    pos_total_ps = pos_total_ct / sent_ct # divided by the number of sentences\n",
    "    jj_total_ct = get_dict_value_ct(jj_dict) # number of adjective categories present in the text\n",
    "    jj_total_sum = get_dict_value_sum(jj_dict) # number of total adjectives present in the text\n",
    "    jj_density = jj_total_sum / token_ct\n",
    "    nn_total_ct = get_dict_value_ct(nn_dict) # number of noun categories present in the text\n",
    "    nn_total_sum = get_dict_value_sum(nn_dict) # number of total nouns present in the text\n",
    "    nn_density = nn_total_sum / token_ct\n",
    "    prp_total_ct = get_dict_value_ct(prp_dict) # number of pronoun categories present in the text\n",
    "    prp_total_sum = get_dict_value_sum(prp_dict) # number of total pronouns present in the text\n",
    "    prp_density = prp_total_sum / token_ct\n",
    "    rb_total_ct = get_dict_value_ct(rb_dict) # number of adverb categories present in the text\n",
    "    rb_total_sum = get_dict_value_sum(rb_dict) # number of total adverbs present in the text\n",
    "    rb_density = rb_total_sum / token_ct\n",
    "    vb_total_ct = get_dict_value_ct(vb_dict) # number of verb categories present in the text\n",
    "    vb_total_sum = get_dict_value_sum(rb_dict) # number of total verbs present in the text\n",
    "    vb_density = vb_total_sum / token_ct\n",
    "    cpx_ct = get_dict_value_ct(cpx_dict) # number of 'complex' categories present in the text\n",
    "    cpx_sum = get_dict_value_sum(cpx_dict) # number of total 'complex' funtion words present in the text\n",
    "    cpx_density = cpx_sum / token_ct\n",
    "\n",
    "    # add 20 aggregate pos features\n",
    "    feature_list.append(pos_total_ct)\n",
    "    feature_list.append(pos_total_ps)\n",
    "    feature_list.append(jj_total_ct)\n",
    "    # feature_list.append(jj_total_sum)\n",
    "    feature_list.append(jj_density)\n",
    "    feature_list.append(nn_total_ct)\n",
    "    # feature_list.append(nn_total_sum)\n",
    "    feature_list.append(nn_density)\n",
    "    feature_list.append(prp_total_ct)\n",
    "    # feature_list.append(prp_total_sum)\n",
    "    feature_list.append(prp_density)\n",
    "    feature_list.append(rb_total_ct)\n",
    "    # feature_list.append(rb_total_sum)\n",
    "    feature_list.append(rb_density)\n",
    "    feature_list.append(vb_total_ct)\n",
    "    # feature_list.append(vb_total_sum)\n",
    "    feature_list.append(vb_density)\n",
    "    feature_list.append(cpx_ct)\n",
    "    # feature_list.append(cpx_sum)\n",
    "    feature_list.append(cpx_density)\n",
    "\n",
    "    # get cognate feature\n",
    "    # L1 is an argument, default = es\n",
    "\n",
    "    language_codes = {\"es\": \"Spanish\", \"fr\": \"French\", \"ht\": \"Haitian Creole\", \"por\": \"Portuguese\"}\n",
    "    cognate_dictionaries = {\"es\": en_es_dict, \"fr\": en_fr_dict, \"ht\": en_ht_dict, \"por\": en_por_dict}\n",
    "    cognate_dict = cognate_dictionaries.get(L1_val)\n",
    "\n",
    "    cognate_token_pct = get_cognate_pct(tokens, cognate_dict)\n",
    "    cognate_type_pct = get_cognate_pct(types, cognate_dict)\n",
    "    \n",
    "    feature_list.append(cognate_token_pct)\n",
    "    feature_list.append(cognate_type_pct)\n",
    "    \n",
    "    # get legacy readability indices:\n",
    "\n",
    "    fkre = flesch_kincaid(text)\n",
    "    gf = gunning_fog(text)\n",
    "    ari = get_ari(text)\n",
    "    cl = coleman_liau(text)\n",
    "\n",
    "    feature_list.append(fkre)\n",
    "    feature_list.append(gf)\n",
    "    feature_list.append(ari)\n",
    "    feature_list.append(cl)\n",
    "    \n",
    "    # get and append 4 'general service list' word frequency features\n",
    "\n",
    "    gsl_token_ct = gsl_simple_ct(tokens, my_gsl_dict)\n",
    "    gsl_type_ct = gsl_simple_ct(types, my_gsl_dict)\n",
    "    gsl_token_freq = get_gsl_freq(tokens, my_gsl_dict)\n",
    "    gsl_type_freq = get_gsl_freq(types, my_gsl_dict)\n",
    "\n",
    "    gsl_pct_tokens = gsl_token_ct / token_ct\n",
    "    gsl_pct_types = gsl_type_ct / type_ct\n",
    "    gsl_dfreq_tokens = gsl_token_freq / token_ct\n",
    "    gsl_dfreq_types = gsl_type_freq / type_ct\n",
    "    \n",
    "    # gsl_sum_tokens = get_gsl_sum_tokens(tokens, my_gsl_dict, my_gsl_ct_dict)\n",
    "    # gsl_sum_types = get_gsl_sum_types(types, my_gsl_dict, my_gsl_ct_dict)\n",
    "    # gsl_d_tokens = get_gsl_density_tokens(gsl_sum_tokens, token_ct)\n",
    "    # log_gsl_d_tokens = get_log_gsl_dens_tokens(gsl_sum_tokens, token_ct)\n",
    "    # gsl_d_types = get_gsl_density_types(gsl_sum_types, type_ct)\n",
    "    # log_gsl_d_types = get_log_gsl_dens_types(gsl_sum_types, type_ct)\n",
    "\n",
    "    feature_list.append(gsl_pct_tokens)\n",
    "    feature_list.append(gsl_pct_types)\n",
    "    feature_list.append(gsl_dfreq_tokens)\n",
    "    feature_list.append(gsl_dfreq_types)\n",
    "\n",
    "    # get and append two academic word list features from eap\n",
    "    \n",
    "    acad_token_ct = acad_ct(tokens, my_acad_dict)\n",
    "    acad_type_ct = acad_ct(types, my_acad_dict)\n",
    "\n",
    "    acad_pct_tokens = acad_token_ct / token_ct\n",
    "    acad_pct_types = acad_type_ct / type_ct\n",
    "\n",
    "    feature_list.append(acad_pct_tokens)\n",
    "    feature_list.append(acad_pct_types)\n",
    "\n",
    "    # get and append two word frequency features from longman 3000\n",
    "    \n",
    "    longman_token_freq = get_longman_freq(tokens, longman_dict)\n",
    "    longman_type_freq = get_longman_freq(types, longman_dict)\n",
    "    longman_pct_tokens = longman_token_freq / token_ct\n",
    "    longman_pct_types = longman_type_freq / type_ct\n",
    "\n",
    "    feature_list.append(longman_pct_tokens)\n",
    "    feature_list.append(longman_pct_types)\n",
    "    \n",
    "    return feature_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002cb1a2-e9dc-4a93-8b96-56b4849e4822",
   "metadata": {},
   "source": [
    "At this point, in the main notebook, it was time to train the classifier. Instead we are going to load the trained classifier and use it to make a prediction on a given sample. Also outputting the other lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5934a2f9-0639-4f6d-b929-ecef90c95c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the trained model\n",
    "loaded_clf = joblib.load(\"./classifier_v4.joblib\")\n",
    "# load the current text\n",
    "\n",
    "def eval_output(text, loaded_clf):\n",
    "    features = get_features(text)\n",
    "    feature_array = np.array(features)\n",
    "    sample = feature_array.reshape(1, -1)\n",
    "    my_y_value = loaded_clf.predict(sample)\n",
    "    if my_y_value == 0:\n",
    "        output_level = \"beginner\"\n",
    "    elif my_y_value == 1:\n",
    "        output_level = \"intermediate\"\n",
    "    else:\n",
    "        output_level = \"advanced\"\n",
    "    eval_str = f\"This text is best suited for {output_level}-level ML students.\"\n",
    "    return eval_str\n",
    "\n",
    "def cognate_output(text, L1_val=\"es\"):\n",
    "    language_codes = {\"es\": \"Spanish\", \"fr\": \"French\", \"ht\": \"Haitian Creole\", \"por\": \"Portuguese\"}\n",
    "    cognate_dictionaries = {\"es\": en_es_dict, \"fr\": en_fr_dict, \"ht\": en_ht_dict, \"por\": en_por_dict}\n",
    "    cognate_dict = cognate_dictionaries.get(L1_val)\n",
    "    cognate_list = get_cognate_list(text, cognate_dict)\n",
    "    sorted_cognate_list = sorted(cognate_list)\n",
    "    L1_name = language_codes.get(L1_val)\n",
    "    header_string = f\"Cognates for {L1_name} present in this text:\"\n",
    "    return header_string, sorted_cognate_list, L1_name\n",
    "\n",
    "def acad_output(text, acad_dict):\n",
    "    tokens = get_tokens(text)\n",
    "    acad_list = acad_in_list(tokens, acad_dict)\n",
    "    return acad_list\n",
    "\n",
    "def main(input_filepath, L1_val=\"es\", acad_dict=my_acad_dict):\n",
    "    output_filepath = input_filepath[:-4] + \"_output.txt\"\n",
    "    text = process_file(input_filepath)\n",
    "    eval_str = eval_output(text, loaded_clf)\n",
    "    cognate_header, cognate_list, L1_name = cognate_output(text, L1_val)\n",
    "    acad_list = acad_output(text, acad_dict)\n",
    "    with open(output_filepath, \"w\", encoding=\"utf-8\") as file:\n",
    "        print(eval_str, file=file)\n",
    "        if len(cognate_list) > 1:\n",
    "            print(\" \", file=file)\n",
    "            print(cognate_header, file=file)\n",
    "            print(\" \", file=file)\n",
    "            print(\"{:>16s}   {:<16s}\".format(\"English\",L1_name), file=file) # Later this needs to be \"L1 name\" variable\n",
    "            print(\"{:>16s}   {:<16s}\".format(\"----------\",\"----------\"), file=file)\n",
    "            for item in cognate_list:\n",
    "                line_str = \"{:>16s} - {:<16s}\".format(item[0],item[1])\n",
    "                print(line_str, file=file)\n",
    "        if len(acad_list) > 1:\n",
    "            print(\" \", file=file)\n",
    "            print(\"Academic vocabulary from the text:\", file=file)\n",
    "            print(\" \", file=file)\n",
    "            for item in acad_list:\n",
    "                print(item, file=file)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d54cc216-8b40-45c0-854c-a24c0d63c382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main(\"test.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
